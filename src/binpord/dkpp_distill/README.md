# Knowledge distillation as label smoothing

This experiment along with the TALS one are aimed at viewing the KD as a label smoothing technique. This particular experiment is made according to the description in the [Born Again Networks paper](https://arxiv.org/abs/1805.04770).

Full results can be found in [this report](https://wandb.ai/binpord/kd-cifar100-resnet18/reports/Knowledge-distillation-as-label-smoothing--VmlldzoxODY0MDQx?accessToken=03x5bwneljau2b8kh0mzg4bttxxtp7fwbfeo03vtw5zl6a2frbq56wckkmdh5q9u).

**TL;DR**: the DKPP approaches both failed to even match the baseline (however, non-persistent DKPP came close). TALS approach matched the baseline, but failed to improve it.
