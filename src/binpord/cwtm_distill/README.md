# Knowledge distillation as importance weighting

This experiment along with the CWTC one follow the ideas presented in [Born Again Networks paper](https://arxiv.org/abs/1805.04770) and interpret the KD as the importance weighting.

All details and results can be found in [this report](https://wandb.ai/binpord/kd-cifar100-resnet18/reports/Knowledge-distillation-as-importance-weighting--VmlldzoxODQwMzQ3?accessToken=w7f4r5c48m9w7t88610fdlpqa97qxxacvlpif3fgbw6mx3d3ng82bn8cvs0r466l).

**TL;DR**: importance weighting does not yield better models, however it seems to yield models with similar errors on validation, but the later is not clear and requires some investigating.
